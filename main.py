import os
import json
import logging
import requests
import time
import datetime
import sys
import re
from bs4 import BeautifulSoup
from groq import Groq
from typing import Dict, Any, Optional, Tuple

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è GitHub Actions (–≤—ã–≤–æ–¥ —Å—Ä–∞–∑—É –≤ –∫–æ–Ω—Å–æ–ª—å)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', stream=sys.stdout)
logger = logging.getLogger("Tracen_Intelligence")

# –†–æ–ª–∏ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
ROLE_NEWS = "1440444308506280210"
ROLE_BANNER = "1439787310831894679"

# –ö–æ–Ω—Ñ–∏–≥–∏ URL
JP_URL = "https://umamusume.jp/news/"
GLOBAL_URL = "https://www.crunchyroll.com/news" 

DB_JP = "last_id_jp.txt"
DB_GL = "last_id_gl.txt"

GROQ_KEY = os.environ.get("GROQ_API_KEY")
WEBHOOK = os.environ.get("DISCORD_WEBHOOK")

if not GROQ_KEY or not WEBHOOK:
    print("!!! –û–®–ò–ë–ö–ê: –ü—Ä–æ–≤–µ—Ä—å —Å–µ–∫—Ä–µ—Ç—ã –≤ GitHub (GROQ_API_KEY, DISCORD_WEBHOOK) !!!", flush=True)
    sys.exit(1)

client = Groq(api_key=GROQ_KEY)

class TracenScanner:
    def __init__(self, region_name: str, base_url: str, db_file: str):
        self.region = region_name
        self.url = base_url
        self.db_file = db_file
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def get_latest(self) -> Optional[Dict[str, str]]:
        try:
            print(f"--- –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {self.region}... ---", flush=True)
            r = requests.get(self.url, headers=self.headers, timeout=25)
            r.raise_for_status()
            r.encoding = 'utf-8' # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —è–ø–æ–Ω—Å–∫–æ–≥–æ
            soup = BeautifulSoup(r.text, 'html.parser')
            
            if "Japan" in self.region:
                # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ JP –Ω–æ–≤–æ—Å—Ç–µ–π
                item = soup.select_one('.news-list__item') or \
                       soup.find('a', href=re.compile(r'/news/detail\.php\?id=\d+')) or \
                       soup.select_one('li[class*="news"]')
                
                if not item: return None
                
                link_tag = item if item.name == 'a' else item.find('a')
                if not link_tag: return None
                
                href = link_tag['href']
                full_link = "https://umamusume.jp" + href if href.startswith('/') else href
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID
                id_match = re.search(r'id=(\d+)', full_link)
                news_id = id_match.group(1) if id_match else full_link.split('/')[-1]
                
                img_tag = item.find('img')
                img_url = img_tag['src'] if img_tag else None
                
                return {"id": str(news_id), "url": full_link, "img": img_url}
            else:
                # –ü–æ–∏—Å–∫ EN –Ω–æ–≤–æ—Å—Ç–µ–π
                links = soup.find_all('a', href=True)
                for a in links:
                    txt = a.get_text().lower()
                    hrf = a['href'].lower()
                    if "uma-musume" in hrf or "pretty-derby" in hrf:
                        l = a['href'] if a['href'].startswith('http') else "https://www.crunchyroll.com" + a['href']
                        return {"id": str(l.split('/')[-1]), "url": l, "img": None}
                return None
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–µ—Ä–∞ {self.region}: {e}", flush=True)
            return None

    def check_new(self, current_id: str) -> bool:
        if not os.path.exists(self.db_file): return True
        with open(self.db_file, 'r') as f:
            return f.read().strip() != current_id

    def save_id(self, current_id: str):
        with open(self.db_file, 'w') as f: f.write(str(current_id))

class MultiRegionAI:
    @staticmethod
    def analyze(html_content: str, region: str) -> Dict[str, Any]:
        print(f"--- –ò–ò –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç {region} ---", flush=True)
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–ª–æ –Ω–æ–≤–æ—Å—Ç–∏, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –ò–ò –Ω–∞ –º–µ–Ω—é —Å–∞–π—Ç–∞
        main_body = soup.select_one('.p-news-detail__body') or \
                    soup.select_one('.news-detail__body') or \
                    soup.select_one('article')
        
        clean_text = main_body.get_text(separator=' ', strip=True) if main_body else soup.get_text()[:4000]

        prompt = f"""
        –¢—ã ‚Äî –≥–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ Tracen Intelligence. –°–¥–µ–ª–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º.
        –†–ï–ì–ò–û–ù: {region}
        –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ —á–∞—Å—Ç–æ –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞.
        
        –í–ï–†–ù–ò –°–¢–†–û–ì–û JSON:
        {{
            "rank": "S/A/B/C", "title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º", "is_banner": bool,
            "summary": "–ö—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å", "details": "–°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π",
            "future": "–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∏–≥—Ä–æ–∫–æ–≤", "verdict": "–°–æ–≤–µ—Ç: –∫—Ä—É—Ç–∏—Ç—å –∏–ª–∏ –∫–æ–ø–∏—Ç—å"
        }}
        –¢–µ–∫—Å—Ç: {clean_text[:5000]}
        """
        try:
            res = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ò–ò: {e}", flush=True)
            return {
                "rank": "B", "title": "–ù–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ", "is_banner": False,
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑.", "details": "–°–º. –æ—Ä–∏–≥–∏–Ω–∞–ª.",
                "future": "N/A", "verdict": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–æ–≤–æ—Å—Ç—å –≤—Ä—É—á–Ω—É—é."
            }

def process_region(region_name, url, db_file):
    scanner = TracenScanner(region_name, url, db_file)
    meta = scanner.get_latest()
    
    if meta and scanner.check_new(meta["id"]):
        print(f"!!! –ù–ê–ô–î–ï–ù–ê –ù–û–í–û–°–¢–¨: {region_name} (ID: {meta['id']}) !!!", flush=True)
        try:
            resp = requests.get(meta["url"], timeout=20)
            resp.encoding = 'utf-8' # –§–ò–ö–° –ö–û–î–ò–†–û–í–ö–ò
            
            analysis = MultiRegionAI.analyze(resp.text, region_name)
            
            ping = f"<@&{ROLE_NEWS}>"
            if analysis.get("is_banner") or analysis.get("rank") == "S":
                ping += f" <@&{ROLE_BANNER}>"
            
            color = {"S": 0xFFD700, "A": 0xFF4500, "B": 0xDA70D6, "C": 0x5DADE2}.get(analysis["rank"], 0x99AAB5)
            
            payload = {
                "content": f"üì¢ **–ù–û–í–´–ô –û–¢–ß–ï–¢: –†–ï–ì–ò–û–ù {region_name.upper()}**\n{ping}",
                "embeds": [{
                    "title": f"‚Äî ‚ú¶ RANK: {analysis.get('rank', 'B')} | {analysis.get('title', 'Update')} ‚ú¶ ‚Äî",
                    "description": (
                        f"**{analysis.get('summary', '')}**\n\n"
                        f"‚ï≠‚îÄ‚îÄ‚îÄ ‚≠ê **–ê–ù–ê–õ–ò–ó ({region_name})**\n"
                        f"‚îÇ {analysis.get('details', '')}\n"
                        "‚îÇ\n"
                        f"‚îÇ ‚ñ∏ **–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï / –°–õ–ò–í–´**\n"
                        f"‚îÇ üîÆ {analysis.get('future', '')}\n"
                        "‚îÇ\n"
                        f"‚îÇ ‚ñ∏ **–í–ï–†–î–ò–ö–¢ –¢–†–ï–ù–ï–†–ê**\n"
                        f"‚îÇ ‚úÖ {analysis.get('verdict', '')}\n"
                        f"‚ï∞‚îÄ‚îÄ‚îÄ üîó [–ò–°–¢–û–ß–ù–ò–ö]({meta['url']})"
                    ),
                    "color": color,
                    "image": {"url": meta["img"]} if meta["img"] else {},
                    "footer": {"text": f"Unit: Tracen Intel ‚Ä¢ Region: {region_name} ‚Ä¢ ID: {meta['id']}"},
                    "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
                }]
            }
            
            if requests.post(WEBHOOK, json=payload).status_code < 300:
                scanner.save_id(meta["id"])
                print(f"–£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Discord.", flush=True)
        except Exception as e:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", flush=True)
    else:
        print(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–π –¥–ª—è {region_name} –Ω–µ—Ç.", flush=True)

if __name__ == "__main__":
    print("=== STARTING TRACEN INTELLIGENCE ===", flush=True)
    process_region("Japan", JP_URL, DB_JP)
    time.sleep(5)
    process_region("Global", GLOBAL_URL, DB_GL)
    print("=== WORK FINISHED ===", flush=True)
