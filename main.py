import os
import json
import logging
import requests
import time
import datetime
import sys
import re
from bs4 import BeautifulSoup
from groq import Groq
from typing import Dict, Any, Optional, Tuple

# ==============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –°–ò–°–¢–ï–ú–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
# ==============================================================================

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("Tracen_Intelligence_Center")

# ID –†–æ–ª–µ–π –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
ROLE_NEWS = "1440444308506280210"
ROLE_BANNER = "1439787310831894679"

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
JP_URL = "https://umamusume.jp/news/"
# –°—Ç–∞–±–∏–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è EN-–Ω–æ–≤–æ—Å—Ç–µ–π (–æ–±—Ö–æ–¥–∏—Ç NameResolutionError)
GLOBAL_URL = "https://www.crunchyroll.com/news" 

DB_JP = "last_id_jp.txt"
DB_GL = "last_id_gl.txt"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–∫—Ä–µ—Ç–æ–≤
GROQ_KEY = os.environ.get("GROQ_API_KEY")
WEBHOOK = os.environ.get("DISCORD_WEBHOOK")

if not GROQ_KEY or not WEBHOOK:
    logger.critical("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ–∫—Ä–µ—Ç—ã (GROQ_API_KEY –∏ DISCORD_WEBHOOK) –≤ GitHub!")
    sys.exit(1)

client = Groq(api_key=GROQ_KEY)

# ==============================================================================
# –ú–û–î–£–õ–¨ –ì–ò–ë–ö–û–ì–û –°–ö–ê–ù–ï–†–ê
# ==============================================================================

class TracenScanner:
    def __init__(self, region: str, url: str, db: str):
        self.region = region
        self.url = url
        self.db = db
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def get_latest(self) -> Optional[Dict[str, str]]:
        try:
            r = requests.get(self.url, headers=self.headers, timeout=25)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, 'html.parser')
            
            if "Japan" in self.region:
                # –ü–∞—Ä—Å–∏–Ω–≥ —è–ø–æ–Ω—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞
                item = soup.select_one('.news-list__item')
                if not item: 
                    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    item = soup.find('a', href=re.compile(r'/news/detail\.php\?id='))
                
                if not item: return None
                
                link_tag = item if item.name == 'a' else item.find('a')
                if not link_tag: return None
                
                link = "https://umamusume.jp" + link_tag['href']
                img_tag = item.find('img') if hasattr(item, 'find') else None
                img = img_tag['src'] if img_tag else None
                news_id = link.split('=')[-1]
                
                return {"id": str(news_id), "url": link, "img": img}
            
            else:
                # –ü–æ–∏—Å–∫ EN-–Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–µ
                links = soup.find_all('a', href=True)
                for a in links:
                    href = a['href'].lower()
                    if "uma-musume" in href or "pretty-derby" in href:
                        full_link = a['href']
                        if not full_link.startswith('http'):
                            full_link = "https://www.crunchyroll.com" + full_link
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL (–ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å –ø—É—Ç–∏)
                        news_id = full_link.rstrip('/').split('/')[-1]
                        return {"id": str(news_id), "url": full_link, "img": None}
                return None

        except Exception as e:
            logger.error(f"[{self.region}] –°–∞–π—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {e}")
            return None

    def is_new(self, news_id: str) -> bool:
        if not os.path.exists(self.db):
            with open(self.db, 'w') as f: f.write("0")
            return True
        with open(self.db, 'r') as f:
            return f.read().strip() != news_id

    def save(self, news_id: str):
        with open(self.db, 'w') as f: f.write(news_id)

# ==============================================================================
# –ò–ò-–ê–ù–ê–õ–ò–¢–ò–ö (LLAMA-3.3-70B)
# ==============================================================================

class MultiRegionAI:
    @staticmethod
    def analyze(raw_html: str, region: str) -> Dict[str, Any]:
        soup = BeautifulSoup(raw_html, 'html.parser')
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ —Å—Ç–∏–ª–µ–π
        for script in soup(["script", "style"]):
            script.decompose()
        text = soup.get_text(separator=' ', strip=True)[:6000]

        prompt = f"""
        –¢—ã ‚Äî –ì–ª–∞–≤–Ω—ã–π –ê–Ω–∞–ª–∏—Ç–∏–∫ –ê–∫–∞–¥–µ–º–∏–∏ –¢—Ä—ç—Å–µ–Ω. –¢–≤–æ—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî Uma Musume.
        –†–ï–ì–ò–û–ù –î–ê–ù–ù–´–•: {region} (–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ).
        
        –¢–í–û–Ø –ó–ê–î–ê–ß–ê:
        1. –ù–∞–∑–Ω–∞—á—å RANK (S/A/B/C) –ø–æ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏.
        2. –ü–µ—Ä–µ–≤–µ–¥–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–∏–π (–∫—Ä–∞—Å–∏–≤–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ).
        3. –≠—Ç–æ –ë–ê–ù–ù–ï–† (–Ω–æ–≤–∞—è –¥–µ–≤–æ—á–∫–∞/–∫–∞—Ä—Ç–∞) –∏–ª–∏ –≤–∞–∂–Ω—ã–π –°–õ–ò–í/–ê–ù–û–ù–°? (True/False).
        4. –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä: –¥–∞—Ç—ã, –Ω–∞–≥—Ä–∞–¥—ã (–∫–∞–º–Ω–∏), –Ω–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∫–∏.
        5. –ü—Ä–æ–≥–Ω–æ–∑: –Ω–∞ —á—Ç–æ —ç—Ç–æ –Ω–∞–º–µ–∫–∞–µ—Ç –≤ –±—É–¥—É—â–µ–º?

        –û–¢–í–ï–¢–¨ –°–¢–†–û–ì–û –í JSON:
        {{
            "rank": "...", "title": "...", "is_banner": bool,
            "summary": "–°—É—Ç—å –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–æ–π", "details": "–°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤",
            "future": "–ß—Ç–æ –∂–¥–∞—Ç—å –¥–∞–ª—å—à–µ?", "verdict": "–°–æ–≤–µ—Ç –∏–≥—Ä–æ–∫—É (–∫—Ä—É—Ç–∏—Ç—å/—Å–∫–∏–ø–∞—Ç—å/–∫–æ–ø–∏—Ç—å)"
        }}
        """
        try:
            res = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ò–ò: {e}")
            return {
                "rank": "B", "title": "–ù–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ", "is_banner": False,
                "summary": "–î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã, –Ω–æ –∞–Ω–∞–ª–∏–∑ –ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.",
                "details": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∏–∂–µ.",
                "future": "N/A", "verdict": "–û–∑–Ω–∞–∫–æ–º—å—Ç–µ—Å—å —Å –Ω–æ–≤–æ—Å—Ç—å—é —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ."
            }

# ==============================================================================
# –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò –ò –û–¢–ü–†–ê–í–ö–ò
# ==============================================================================

def process_region(name: str, url: str, db: str):
    scanner = TracenScanner(name, url, db)
    meta = scanner.get_latest()
    
    if meta and scanner.is_new(meta["id"]):
        logger.info(f"[{name}] –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å! ID: {meta['id']}")
        
        try:
            r = requests.get(meta["url"], timeout=20)
            page_content = r.text
        except:
            page_content = "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏."

        analysis = MultiRegionAI.analyze(page_content, name)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
        ping = f"<@&{ROLE_NEWS}>"
        if analysis.get("is_banner") or analysis.get("rank") == "S":
            ping += f" <@&{ROLE_BANNER}>"
            
        color = {"S": 0xFFD700, "A": 0xFF4500, "B": 0xDA70D6, "C": 0x5DADE2}.get(analysis["rank"], 0x99AAB5)
        
        embed_data = {
            "content": f"üì¢ **–ù–û–í–´–ô –û–¢–ß–ï–¢: {name.upper()}**\n{ping}",
            "embeds": [{
                "title": f"‚Äî ‚ú¶ RANK: {analysis['rank']} | {analysis['title']} ‚ú¶ ‚Äî",
                "description": (
                    f"**{analysis['summary']}**\n\n"
                    f"‚ï≠‚îÄ‚îÄ‚îÄ ‚≠ê **–ê–ù–ê–õ–ò–ó ({name})**\n"
                    f"‚îÇ {analysis['details']}\n"
                    "‚îÇ\n"
                    f"‚îÇ ‚ñ∏ **–ü–†–û–ì–ù–û–ó–´ –ò –°–õ–ò–í–´**\n"
                    f"‚îÇ üîÆ {analysis['future']}\n"
                    "‚îÇ\n"
                    f"‚îÇ ‚ñ∏ **–í–ï–†–î–ò–ö–¢**\n"
                    f"‚îÇ ‚úÖ {analysis['verdict']}\n"
                    f"‚ï∞‚îÄ‚îÄ‚îÄ üîó [–û–†–ò–ì–ò–ù–ê–õ –ù–û–í–û–°–¢–ò]({meta['url']})"
                ),
                "color": color,
                "image": {"url": meta["img"]} if meta["img"] else {},
                "footer": {"text": f"Logic: Llama-3.3-70B ‚Ä¢ Region: {name}"},
                "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
            }]
        }
        
        res = requests.post(WEBHOOK, json=embed_data)
        if res.status_code < 300:
            scanner.save(meta["id"])
            logger.info(f"[{name}] –°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ.")

def main():
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ø–ø–æ–Ω–∏—é
    process_region("Japan", JP_URL, DB_JP)
    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    time.sleep(5)
    # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –ì–ª–æ–±–∞–ª
    process_region("Global", GLOBAL_URL, DB_GL)

if __name__ == "__main__":
    main()
