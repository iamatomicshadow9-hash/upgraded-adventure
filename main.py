import os
import json
import logging
import requests
import time
import datetime
import sys
import re
from bs4 import BeautifulSoup
from groq import Groq
from typing import Dict, Any, Optional, List

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', stream=sys.stdout)
logger = logging.getLogger("Tracen_Intelligence")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Discord
ROLE_NEWS = "1440444308506280210"
ROLE_BANNER = "1439787310831894679"
WEBHOOK = os.environ.get("DISCORD_WEBHOOK")

# –ö–æ–Ω—Ñ–∏–≥–∏ URL
JP_URL = "https://umamusume.jp/news/"
GLOBAL_URL = "https://www.crunchyroll.com/news" 
DB_JP = "last_id_jp.txt"
DB_GL = "last_id_gl.txt"

client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

class TracenScanner:
    def __init__(self, region_name: str, base_url: str, db_file: str):
        self.region = region_name
        self.url = base_url
        self.db_file = db_file
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

    def get_latest_list(self) -> List[Dict[str, str]]:
        try:
            print(f"--- –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {self.region}... ---", flush=True)
            r = requests.get(self.url, headers=self.headers, timeout=25)
            r.encoding = 'utf-8'
            soup = BeautifulSoup(r.text, 'html.parser')
            results = []

            if "Japan" in self.region:
                items = soup.select('.news-list__item')[:3]
                for item in items:
                    link_tag = item.find('a')
                    if not link_tag: continue
                    href = link_tag['href']
                    full_link = "https://umamusume.jp" + href if href.startswith('/') else href
                    id_val = re.search(r'id=(\d+)', full_link).group(1) if "id=" in full_link else full_link.split('/')[-1]
                    img = item.find('img')['src'] if item.find('img') else None
                    results.append({"id": str(id_val), "url": full_link, "img": img})
            else:
                links = soup.find_all('a', href=True)
                for a in links:
                    hrf = a['href'].lower()
                    if ("uma-musume" in hrf or "pretty-derby" in hrf) and len(results) < 2:
                        full_link = a['href'] if a['href'].startswith('http') else "https://www.crunchyroll.com" + a['href']
                        results.append({"id": str(full_link.split('/')[-1]), "url": full_link, "img": None})
            return results
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–µ—Ä–∞ {self.region}: {e}", flush=True)
            return []

    def get_old_ids(self) -> List[str]:
        if not os.path.exists(self.db_file): return []
        with open(self.db_file, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f.readlines() if line.strip()]

    def save_ids(self, ids: List[str]):
        with open(self.db_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(ids))

class MultiRegionAI:
    @staticmethod
    def analyze(html_content: str, region: str) -> Dict[str, Any]:
        soup = BeautifulSoup(html_content, 'html.parser')
        body = soup.select_one('.p-news-detail__body') or soup.select_one('.news-detail__body') or soup.select_one('article')
        text = body.get_text(separator=' ', strip=True) if body else soup.get_text()[:4000]

        prompt = f"–¢—ã –≥–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ Tracen Intelligence. –°–¥–µ–ª–∞–π —Ä–∞–∑–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è {region} –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –í–ï–†–ù–ò –°–¢–†–û–ì–û JSON."
        try:
            res = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏: {text[:5000]}"}],
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content), text
        except:
            return None, text

def process_region(region_name, url, db_file):
    scanner = TracenScanner(region_name, url, db_file)
    latest_news = scanner.get_latest_list()
    old_ids = scanner.get_old_ids()
    
    processed_ids = []
    # –ò–¥–µ–º —Å –∫–æ–Ω—Ü–∞ (–æ—Ç –±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º), —á—Ç–æ–±—ã –≤ Discord –æ–Ω–∏ —à–ª–∏ –≤ –≤–µ—Ä–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    for meta in reversed(latest_news):
        if meta["id"] not in old_ids:
            print(f"!!! –ù–ê–ô–î–ï–ù–ê –ù–û–í–ê–Ø –ù–û–í–û–°–¢–¨: {region_name} (ID: {meta['id']}) !!!", flush=True)
            try:
                resp = requests.get(meta["url"], timeout=20)
                resp.encoding = 'utf-8'
                analysis, raw_text = MultiRegionAI.analyze(resp.text, region_name)
                
                if not analysis: continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∞–≥–∏ –¥–ª—è —Å–º–µ–Ω—ã —Ü–≤–µ—Ç–∞
                is_bug = any(word in raw_text.lower() for word in ["‰∏çÂÖ∑Âêà", "–æ—à–∏–±–∫–∞", "–±–∞–≥", "bug", "–Ω–µ–ø–æ–ª–∞–¥–∫–∞", "‰øÆÊ≠£"])
                color = 0xFF0000 if is_bug else 0xFF69B4 # –ö—Ä–∞—Å–Ω—ã–π –µ—Å–ª–∏ –±–∞–≥, –∏–Ω–∞—á–µ –†–æ–∑–æ–≤—ã–π
                
                ping = f"<@&{ROLE_NEWS}>"
                if is_bug or analysis.get("is_banner") or analysis.get("rank") == "S":
                    ping += f" <@&{ROLE_BANNER}>"
                
                payload = {
                    "content": f"üì¢ **–ù–û–í–´–ô –û–¢–ß–ï–¢: {region_name.upper()}**\n{ping}",
                    "embeds": [{
                        "title": f"‚Äî ‚ú¶ {'‚ö†Ô∏è –ë–ê–ì' if is_bug else 'RANK: ' + analysis.get('rank', 'B')} | {analysis.get('title')} ‚ú¶ ‚Äî",
                        "description": (
                            f"**{analysis.get('summary')}**\n\n"
                            f"‚ï≠‚îÄ‚îÄ‚îÄ ‚≠ê **–ê–ù–ê–õ–ò–ó ({region_name})**\n"
                            f"‚îÇ {analysis.get('details')}\n"
                            "‚îÇ\n"
                            f"‚îÇ ‚ñ∏ **–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï / –°–õ–ò–í–´**\n"
                            f"‚îÇ üîÆ {analysis.get('future')}\n"
                            "‚îÇ\n"
                            f"‚îÇ ‚ñ∏ **–í–ï–†–î–ò–ö–¢ –¢–†–ï–ù–ï–†–ê**\n"
                            f"‚îÇ ‚úÖ {analysis.get('verdict')}\n"
                            f"‚ï∞‚îÄ‚îÄ‚îÄ üîó [–ò–°–¢–û–ß–ù–ò–ö]({meta['url']})"
                        ),
                        "color": color,
                        "image": {"url": meta["img"]} if meta["img"] else {},
                        "footer": {"text": f"Unit: Tracen Intel ‚Ä¢ Region: {region_name} ‚Ä¢ ID: {meta['id']}"},
                        "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
                    }]
                }
                if requests.post(WEBHOOK, json=payload).status_code < 300:
                    processed_ids.append(meta["id"])
                    time.sleep(2)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {meta['id']}: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —É–≤–∏–¥–µ–Ω–Ω—ã–µ ID (–Ω–æ–≤—ã–µ + —Å—Ç–∞—Ä—ã–µ, –¥–µ—Ä–∂–∏–º –ª–∏–º–∏—Ç 15 —à—Ç—É–∫)
    new_db = list(dict.fromkeys([m["id"] for m in latest_news] + old_ids))[:15]
    scanner.save_ids(new_db)

if __name__ == "__main__":
    print("=== –ó–ê–ü–£–°–ö TRACEN PINK SYSTEM ===", flush=True)
    process_region("Japan", JP_URL, DB_JP)
    time.sleep(5)
    process_region("Global", GLOBAL_URL, DB_GL)
    print("=== –ú–û–ù–ò–¢–û–†–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù ===", flush=True)
